{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnUOq3dZeDK1"
      },
      "source": [
        "## RAG System Using Llama2 With Hugging Face"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48c4tcBz8S7u"
      },
      "source": [
        "## Install all libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tHP9Gfafdq57",
        "outputId": "15818357-8604-4862-8d6c-ee9afdef5831"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEPRECATION: Loading egg at c:\\python 3.11\\lib\\site-packages\\vboxapi-1.0-py3.11.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
            "\n",
            "[notice] A new release of pip is available: 23.3.1 -> 24.0\n",
            "[notice] To update, run: C:\\python 3.11\\python.exe -m pip install --upgrade pip\n",
            "DEPRECATION: Loading egg at c:\\python 3.11\\lib\\site-packages\\vboxapi-1.0-py3.11.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
            "\n",
            "[notice] A new release of pip is available: 23.3.1 -> 24.0\n",
            "[notice] To update, run: C:\\python 3.11\\python.exe -m pip install --upgrade pip\n",
            "DEPRECATION: Loading egg at c:\\python 3.11\\lib\\site-packages\\vboxapi-1.0-py3.11.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
            "\n",
            "[notice] A new release of pip is available: 23.3.1 -> 24.0\n",
            "[notice] To update, run: C:\\python 3.11\\python.exe -m pip install --upgrade pip\n",
            "DEPRECATION: Loading egg at c:\\python 3.11\\lib\\site-packages\\vboxapi-1.0-py3.11.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
            "\n",
            "[notice] A new release of pip is available: 23.3.1 -> 24.0\n",
            "[notice] To update, run: C:\\python 3.11\\python.exe -m pip install --upgrade pip\n",
            "DEPRECATION: Loading egg at c:\\python 3.11\\lib\\site-packages\\vboxapi-1.0-py3.11.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
            "\n",
            "[notice] A new release of pip is available: 23.3.1 -> 24.0\n",
            "[notice] To update, run: C:\\python 3.11\\python.exe -m pip install --upgrade pip\n",
            "DEPRECATION: Loading egg at c:\\python 3.11\\lib\\site-packages\\vboxapi-1.0-py3.11.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
            "\n",
            "[notice] A new release of pip is available: 23.3.1 -> 24.0\n",
            "[notice] To update, run: C:\\python 3.11\\python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "!pip install -q pypdf\n",
        "!pip install -q faiss-cpu\n",
        "!pip install -q transformers einops accelerate langchain bitsandbytes\n",
        "!pip install -q sentence_transformers\n",
        "!pip install -q langchain\n",
        "!pip install -q huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tu1g-xwnfv9s",
        "outputId": "ee6eade3-e912-4b66-8eed-cb4e34edf53f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEPRECATION: Loading egg at c:\\python 3.11\\lib\\site-packages\\vboxapi-1.0-py3.11.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
            "\n",
            "[notice] A new release of pip is available: 23.3.1 -> 24.0\n",
            "[notice] To update, run: C:\\python 3.11\\python.exe -m pip install --upgrade pip\n",
            "DEPRECATION: Loading egg at c:\\python 3.11\\lib\\site-packages\\vboxapi-1.0-py3.11.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
            "\n",
            "[notice] A new release of pip is available: 23.3.1 -> 24.0\n",
            "[notice] To update, run: C:\\python 3.11\\python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "## Embedding\n",
        "!pip install -q install sentence_transformers\n",
        "!pip install -q llama-index-embeddings-langchain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SMiIH6t34CFn",
        "outputId": "ddefdc70-b1bb-4bca-91ea-c42db035fb14"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEPRECATION: Loading egg at c:\\python 3.11\\lib\\site-packages\\vboxapi-1.0-py3.11.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
            "\n",
            "[notice] A new release of pip is available: 23.3.1 -> 24.0\n",
            "[notice] To update, run: C:\\python 3.11\\python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "# Used for reading pdf files\n",
        "!pip install -q llama_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HutpiHgpgLBE"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import VectorStoreIndex,SimpleDirectoryReader,ServiceContext,PromptTemplate\n",
        "# from llama_index.llms.huggingface import HuggingFaceLLM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f68QX2MZ8dk7"
      },
      "source": [
        "## Load your pdf files for rag in the data folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "id": "LHD7YnJ7Su-w",
        "outputId": "c7b39bfe-a830-4b64-abca-c029ae1e59d6"
      },
      "outputs": [
        {
          "ename": "MessageError",
          "evalue": "Error: credential propagation was unsuccessful",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-d5df0069828e>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    131\u001b[0m   )\n\u001b[1;32m    132\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    134\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ro7cUIipZsv0",
        "outputId": "e0874c19-e024-48e7-f73b-57933fc0cfd8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘data’: File exists\n"
          ]
        }
      ],
      "source": [
        "# Create a folder data\n",
        "!mkdir data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l6kjJ31i_F5p",
        "outputId": "7a3461c3-73be-4433-89bd-e63e15672429"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: langchain_community in /usr/local/lib/python3.10/dist-packages (0.2.0)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (2.0.30)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (3.9.5)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.6.6)\n",
            "Requirement already satisfied: langchain<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.2.0)\n",
            "Requirement already satisfied: langchain-core<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.2.0)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.1.60)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (1.25.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (8.3.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (4.0.3)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.21.2)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
            "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from langchain<0.3.0,>=0.2.0->langchain_community) (0.2.0)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain<0.3.0,>=0.2.0->langchain_community) (2.7.1)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.0->langchain_community) (1.33)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.0->langchain_community) (23.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.0->langchain_community) (3.10.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (2024.2.2)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (4.11.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.0.3)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.0->langchain_community) (2.4)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.0->langchain_community) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.0->langchain_community) (2.18.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.0.0)\n"
          ]
        }
      ],
      "source": [
        "! pip install langchain_community"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XIEZltGJY6C9"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "# from llama_index.core import SimpleDirectoryReader\n",
        "\n",
        "# docs = PyPDFLoader('/content/drive/MyDrive/data/Viewing Ocean from Space.pdf').load()\n",
        "# docs = PyPDFLoader('/content/drive/MyDrive/data/Advt_JRF_Chemistry_MoHUA_23.pdf').load()\n",
        "# docs = PyPDFLoader('/content/drive/MyDrive/data/A.pdf').load()\n",
        "# docs3 = PyPDFLoader('/content/drive/MyDrive/data/PG_2023-24.pdf').load()\n",
        "# docs4 = PyPDFLoader('/content/drive/MyDrive/data/PG_Manual2022.pdf').load()\n",
        "# docs = PyPDFLoader('/content/Glimpses2018.pdf').load()\n",
        "docs = PyPDFLoader('/content/drive/MyDrive/data/Notice.pdf').load()\n",
        "# docs = PyPDFLoader('/content/Space India Jan-Jun 2017.pdf').load()\n",
        "# docs6 = PyPDFLoader('/content/drive/MyDrive/data/PhD_2023-24.pdf').load()\n",
        "# docs7 = PyPDFLoader('/content/drive/MyDrive/data/Shortlist_candidates_for_Stage_II_III_for_the_post_of_Assistant_Registrar.pdf').load()\n",
        "# docs = PyPDFLoader('/content/drive/MyDrive/data/UG_2023-24.pdf').load()\n",
        "# docs = SimpleDirectoryReader('/content/data').load_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zpjEchlm9EFx"
      },
      "source": [
        "## Setup your huggingface token session"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "S5-mOhoIhfo0",
        "outputId": "b6d29c80-7622-4108-a636-919aac2bfefe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    A token is already saved on your machine. Run `huggingface-cli whoami` to get more information or `huggingface-cli logout` if you want to log out.\n",
            "    Setting a new token will erase the existing one.\n",
            "    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Token: "
          ]
        }
      ],
      "source": [
        "!huggingface-cli login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bodnZ9EVLvxO"
      },
      "outputs": [],
      "source": [
        "!pip3 install Cython"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q94hzt_09IfT"
      },
      "source": [
        "## Download llama 2 model and hkunlp embedding model\n",
        "\n",
        "- Run this if you want to do everything on local machine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nlhRrEbRMo3g"
      },
      "outputs": [],
      "source": [
        "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
        "\n",
        "embed_model=HuggingFaceEmbeddings(model_name=\"hkunlp/instructor-large\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r5TLvKb0LrvA"
      },
      "outputs": [],
      "source": [
        "# Create vector database using hkunlp embeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "\n",
        "text_splitter = CharacterTextSplitter(separator=' ', chunk_size=1024, chunk_overlap=200)\n",
        "documents = text_splitter.split_documents(docs)\n",
        "index = FAISS.from_documents(documents, embed_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vl39KC4wUbN-"
      },
      "outputs": [],
      "source": [
        "!pip install -i https://pypi.org/simple/ bitsandbytes\n",
        "!pip install accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jyVOhSuhhqdb"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import transformers\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\",\n",
        "                                          use_auth_token=True,)\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\",\n",
        "                                            #  device_map='auto',\n",
        "                                             torch_dtype=torch.float16,\n",
        "                                            #  use_auth_token=True,\n",
        "                                            #  load_in_8bit=True,\n",
        "                                            #  load_in_4bit=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hLleDSFsedYX"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "from langchain import HuggingFacePipeline\n",
        "\n",
        "pipe = pipeline(\"text-generation\",\n",
        "                model=model,\n",
        "                tokenizer= tokenizer,\n",
        "                torch_dtype=torch.bfloat16,\n",
        "                device_map=\"auto\",\n",
        "                max_new_tokens = 512,\n",
        "                do_sample=True,\n",
        "                top_k=30,\n",
        "                num_return_sequences=1,\n",
        "                eos_token_id=tokenizer.eos_token_id\n",
        "                )\n",
        "\n",
        "llm = HuggingFacePipeline(pipeline = pipe, model_kwargs = {'temperature':0.35})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qH2yu7r_KrZ-"
      },
      "source": [
        "# OpenAI Embeddings\n",
        "\n",
        "- Only run this if you want to use openAI embeddings instead of hkunlp embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6eaTJmv2KzWi"
      },
      "outputs": [],
      "source": [
        "!pip install -q langchain-openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nhzc5BOOKzUO"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "\n",
        "embed_model = OpenAIEmbeddings(api_key = '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')\n",
        "\n",
        "text_splitter = CharacterTextSplitter(separator=' ', chunk_size=1024, chunk_overlap=200)\n",
        "documents = text_splitter.split_documents(docs)\n",
        "index = FAISS.from_documents(documents, embed_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_xZHYSkQpfB"
      },
      "source": [
        "# prompts and chat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LaZYGgL0VrTY"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import textwrap\n",
        "\n",
        "B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
        "B_SYS, E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\"\n",
        "DEFAULT_SYSTEM_PROMPT = \"\"\"\\\n",
        "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
        "\n",
        "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "def get_prompt(instruction, new_system_prompt=DEFAULT_SYSTEM_PROMPT ):\n",
        "    SYSTEM_PROMPT = B_SYS + new_system_prompt + E_SYS\n",
        "    prompt_template =  B_INST + SYSTEM_PROMPT + instruction + E_INST\n",
        "    return prompt_template\n",
        "\n",
        "\n",
        "def parse_text(text):\n",
        "        wrapped_text = textwrap.fill(text, width=100)\n",
        "        print(wrapped_text +'\\n\\n')\n",
        "        # return assistant_text\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bTsDkfJet_NL"
      },
      "source": [
        "## using together ai for llama2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6r4dNYRYekvo"
      },
      "outputs": [],
      "source": [
        "pip install -U langchain-together"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HRxUwZcbXcMT"
      },
      "outputs": [],
      "source": [
        "from langchain_together import Together\n",
        "\n",
        "llm = Together(\n",
        "    model=\"meta-llama/Llama-3-70b-chat-hf\",\n",
        "    temperature=0.35,\n",
        "    max_tokens=300,\n",
        "    top_k=2,\n",
        "    together_api_key=\"21003311b33c7dddadb89d6c0c7b42e81d8e39c866f672692b15a404e32351e6\",\n",
        "    verbose=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2XtHXszLe4PC"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "template = \"You are an AI chatbot, who specializes in football, and knows everything there is to know about football. Now talk with a human. \\nHuman: {user_message}\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8AxBHDVbgqQX"
      },
      "outputs": [],
      "source": [
        "chain = prompt | llm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lAXSryKDe8M8"
      },
      "outputs": [],
      "source": [
        "print(chain.invoke('<s>[INST]hello, how are you? [/INST] </s>'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xs9yqdw2g4bY"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2P4Ygn5cg4Zi"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XWunU62yg4Ws"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CSgPpIwfXcCy"
      },
      "outputs": [],
      "source": [
        "retriever = index.as_retriever(search_type=\"similarity_score_threshold\",\n",
        "                                       search_kwargs={\"k\": 1, \"score_threshold\": 0.7})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xaYuhusPM_hY"
      },
      "outputs": [],
      "source": [
        "query = 'Tell me about hedging gains'\n",
        "docs = retriever.get_relevant_documents(query)\n",
        "print(docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mNhliP1xe__z"
      },
      "outputs": [],
      "source": [
        "prompt = ''\n",
        "i = 1\n",
        "chat_history = []\n",
        "while ((query := input('enter query: (q to exit)')) != 'q'):\n",
        "  docs = retriever.get_relevant_documents(query)\n",
        "  context = \"\"\n",
        "  if docs:\n",
        "    context = docs[0].page_content\n",
        "\n",
        "  sys_prompt = f\"\"\"<s>[INST]You are a helpful AI chatbot conversing with a human. Use the following pieces of context to \\\n",
        "answer the users question. \\\n",
        "Only use the context to answer the human's question. \\\n",
        "If you don't know the answer, just say that you don't know, don't try to make up an answer.\\n\\n\"\"\"\n",
        "\n",
        "  # prompt = chat_history\n",
        "  history = ''.join(chat_history[-5:])\n",
        "  prompt = history + f\"\"\"{sys_prompt}Context: {context}\\n\\nHuman: {query}\\n[/INST]\"\"\"\n",
        "  # print(f'this is chat : {i}\"s prompt: \\n{prompt}\\n')\n",
        "  i += 1\n",
        "  response = llm.invoke(prompt)\n",
        "  # response = 'This is the answer'\n",
        "  print(response)\n",
        "  print('*' * 200)\n",
        "  response += '</s>\\n'\n",
        "  query = f'<s>[INST]Human: {query}[/INST]\\n'\n",
        "  response = f'AI: {response}'\n",
        "  chat_history.append(query)\n",
        "  chat_history.append(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_HJGvt2XuJLN"
      },
      "source": [
        "## use downloaded model for llama2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ajOWosgS5Azu"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain_core.prompts import PromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate, ChatPromptTemplate\n",
        "\n",
        "def chat_pdf_bot(question, chat_history, index):\n",
        "\n",
        "  sys_prompt = \"\"\"[INST]<<SYS>>\\nYou are a helpful AI chatbot conversing with a human. Use the following pieces of context to answer the users question. \\\n",
        "  Only use the context if it is relevant to the human's question. \\\n",
        "  If you don't know the answer, just say that you don't know, don't try to make up an answer. \\\n",
        "  ----------------\\\n",
        "  {context}\\n<</SYS>>\\n\\n\"\"\"\n",
        "\n",
        "  prompt_template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
        "\n",
        " {context}\n",
        "\n",
        " Question: {question}\n",
        " Helpful Answer:[/INST]\"\"\"\n",
        "\n",
        "  PROMPT = PromptTemplate(\n",
        "     template=prompt_template, input_variables=[\"context\", \"question\"]\n",
        " )\n",
        "\n",
        "  messages = [\n",
        "     SystemMessagePromptTemplate.from_template(sys_prompt),\n",
        "     HumanMessagePromptTemplate.from_template(\"{question}\"),\n",
        " ]\n",
        "\n",
        "  your_prompt = ChatPromptTemplate.from_messages(messages)\n",
        "\n",
        "  retriever = index.as_retriever(search_type=\"similarity_score_threshold\",\n",
        "                                       search_kwargs={\"k\":2, \"score_threshold\": 0.8})\n",
        "  qa = ConversationalRetrievalChain.from_llm(llm, retriever, verbose=True, combine_docs_chain_kwargs={\"prompt\": your_prompt})\n",
        "\n",
        "  result = qa({\"question\":question,\"chat_history\":chat_history})\n",
        "  return result['answer']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9xt9ym1u5AtU"
      },
      "outputs": [],
      "source": [
        "chat_history = []\n",
        "while (query := input(\"enter your question: \")) != 'q':\n",
        "  response = chat_pdf_bot(query, chat_history, index)\n",
        "  print(response)\n",
        "  response += '[/INST]\\n'\n",
        "  chat_history.append((query, response))\n",
        "  print(\"*\" * 200)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9Wz0B0XVolG"
      },
      "source": [
        "#Experimental"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wFP8iejXVuI6"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "\n",
        "system_prompt = \"You are a helpful AI assistant. Answer questions asked by the user based only on the provided context. If context is not provided, simply say 'I don't know. Also use the provided chat history to get context if needed.'\"\n",
        "instructions = \"Context: {context} \\n\\nChat_History: {chat_history} \\n\\nUser_input: {input}\"\n",
        "prompt_template = get_prompt(instructions, system_prompt)\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(prompt_template)\n",
        "print(prompt)\n",
        "\n",
        "document_chain = create_stuff_documents_chain(llm, prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y5Nn6GaRQrhN"
      },
      "outputs": [],
      "source": [
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "from langchain.chains import create_retrieval_chain\n",
        "\n",
        "retriever = index.as_retriever(search_kwargs={\"score_threshold\": 0.75, \"k\": 2})\n",
        "retrieval_chain = create_retrieval_chain(retriever, document_chain)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "32kbGhQFVjz-"
      },
      "outputs": [],
      "source": [
        "while (query := input(\"Enter your query (q to exit): \")) != 'q':\n",
        "  response = retrieval_chain.invoke({\n",
        "      'input': query})\n",
        "  print(response['answer'])\n",
        "  print('\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "apBfhs5rVjxc"
      },
      "outputs": [],
      "source": [
        "\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V_-kUd4ZVjvG"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Avy4iklVjC_"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "akdpM1faeWtj"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_iswjnAbeWlg"
      },
      "outputs": [],
      "source": [
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain import LLMChain, PromptTemplate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rSr8R_ujfmn2"
      },
      "outputs": [],
      "source": [
        "instruction = \"Chat History:\\n\\n{chat_history} \\n\\nUser: {user_input}\"\n",
        "system_prompt = \"You are a helpful assistant, you always only answer for the assistant then you stop. read the chat history to get context\"\n",
        "\n",
        "template = get_prompt(instruction, system_prompt)\n",
        "print(template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "srmhkGg_fmlN"
      },
      "outputs": [],
      "source": [
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"chat_history\", \"user_input\"], template=template\n",
        ")\n",
        "memory = ConversationBufferMemory(memory_key=\"chat_history\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QYkWA_1Tfmi3"
      },
      "outputs": [],
      "source": [
        "llm_chain = LLMChain(\n",
        "    llm=llm,\n",
        "    prompt=prompt,\n",
        "    verbose=True,\n",
        "    memory=memory,\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
